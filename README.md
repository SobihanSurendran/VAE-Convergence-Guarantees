# Theoretical Convergence Guarantees for Variational Autoencoders

## Overview
This repository contains the code and experiments for the paper "Theoretical Convergence Guarantees for Variational Autoencoders." The paper provides non-asymptotic convergence rates for Variational Autoencoders trained using Stochastic Gradient Descent and Adam. It includes implementations of VAE, $\beta$-VAE, and IWAE with Adam on the CIFAR-100 and CelebA datasets.

## Installation
To set up the environment, install the required dependencies using the following command:

```bash
pip install -r requirements.txt
```


## Citations
If you find this repository valuable for your research, please cite our work using the following references:

- Surendran, S., Godichon-Baggioni, A., & Le Corff S. (2024). Theoretical Convergence Guarantees for Variational Autoencoders. *arXiv preprint arXiv:2410.16750*.

```bibtex
  @article{surendran2024theoretical,
  title={Theoretical Convergence Guarantees for Variational Autoencoders},
  author={Surendran, Sobihan and Godichon-Baggioni, Antoine and Le Corff, Sylvain},
  journal={arXiv preprint arXiv:2410.16750},
  year={2024}
}
```
